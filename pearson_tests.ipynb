{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from consolidated_runs import run_simulations\n",
    "\n",
    "from Pearson.pearson import Pearson\n",
    "\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_with_proportion(total, targets, proportion):\n",
    "    total_sample = 10\n",
    "    num_special = int(total_sample * proportion)\n",
    "    special = np.random.choice(targets, num_special, replace=False)\n",
    "\n",
    "    remaining = list(set(total) - set(targets))\n",
    "    normal = np.random.choice(remaining, total_sample - num_special, replace=False)\n",
    "\n",
    "    final = np.concatenate([special, normal])\n",
    "    np.random.shuffle(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = range(1, 4) # DS3\n",
    "run_simulations(datasets,\n",
    "            sergio=True,\n",
    "            saucie=True, \n",
    "            scScope=True, \n",
    "            deepImpute=True, \n",
    "            magic=True, \n",
    "            genie=False,\n",
    "            arboreto=False,\n",
    "            pearson=False,\n",
    "            roc=False,\n",
    "            precision_recall_k=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_dir = os.path.join(os.getcwd(), 'imputations')\n",
    "\n",
    "def run_pearson_ranking(proportion):\n",
    "    print(\"Running Pearson ranking tests with proportion\", proportion)\n",
    "    for i in range(1, 4):\n",
    "        gt_file = None\n",
    "        if i == 1:\n",
    "            gt_file = './SERGIO/data_sets/De-noised_100G_9T_300cPerT_4_DS1/gt_GRN.csv'\n",
    "        elif i == 2:\n",
    "            gt_file = './SERGIO/data_sets/De-noised_400G_9T_300cPerT_5_DS2/gt_GRN.csv'\n",
    "        elif i == 3:\n",
    "            gt_file = 'SERGIO/data_sets/De-noised_1200G_9T_300cPerT_6_DS3/gt_GRN.csv'\n",
    "        \n",
    "        # For now, just load clean data, save as csv, and load it back in\n",
    "        methods = ['Clean', 'Noisy', 'SAUCIE', 'scScope', 'DeepImpute', 'MAGIC']\n",
    "        file_name = ''\n",
    "        save_name = ''\n",
    "        for method in methods:\n",
    "            if method == 'Clean':\n",
    "                file_name = f'DS6_clean.npy'\n",
    "                save_name = f'DS6_clean.csv'\n",
    "            elif method == 'Noisy':\n",
    "                file_name = f'DS6_45.npy'\n",
    "                save_name = f'DS6_45.csv'\n",
    "            elif method == 'SAUCIE':\n",
    "                file_name = f'yhat_SAUCIE.npy'\n",
    "                save_name = f'yhat_SAUCIE.csv'\n",
    "            elif method == 'scScope':\n",
    "                file_name = f'yhat_scScope.npy'\n",
    "                save_name = f'yhat_scScope.csv'\n",
    "            elif method == 'DeepImpute':\n",
    "                file_name = f'yhat_deepImpute.npy'\n",
    "                save_name = f'yhat_deepImpute.csv'\n",
    "            elif method == 'MAGIC':\n",
    "                file_name = f'yhat_MAGIC_t_auto.npy'\n",
    "                save_name = f'yhat_MAGIC_t_auto.csv'\n",
    "            load_dir = os.path.join(imp_dir, f'DS{i}')\n",
    "            if not os.path.exists(os.path.join(imp_dir, f'DS{i}', save_name)):\n",
    "                npfile = np.load(load_dir + '/' + file_name)\n",
    "                df = pd.DataFrame(npfile)\n",
    "                df.to_csv(load_dir + '/' + save_name, index=False)\n",
    "            clean_df = pd.read_csv(os.path.join(load_dir, save_name))\n",
    "            \n",
    "            # Read ground truth\n",
    "            gt = pd.read_csv(gt_file, header=None)\n",
    "            confirmed_genes = gt[0].unique()\n",
    "\n",
    "            # Run Pearson on clean and noisy data\n",
    "            pearson = Pearson(np.transpose(clean_df), '')\n",
    "            p_values = pearson.values\n",
    "            np.fill_diagonal(p_values, 0)\n",
    "            pearson = pd.DataFrame(p_values, index=pearson.columns, columns=pearson.columns)\n",
    "\n",
    "            total = 0\n",
    "            for r in range(50):\n",
    "                sampled = sample_with_proportion(clean_df.index.tolist(), confirmed_genes, proportion)\n",
    "            # Take subset of pearson data with only the sampled genes\n",
    "                p_subset = pearson.loc[sampled]\n",
    "            #print(p_subset)\n",
    "            # Rank the expression pairs by the absolute value of the pearson correlation\n",
    "                melted = pd.melt(p_subset.reset_index(), id_vars=['index'], value_vars=p_subset.columns)\n",
    "                melted.columns = ['input', 'target', 'correlation']\n",
    "                melted_sorted = melted.sort_values(by='correlation', ascending=False)\n",
    "                ranked_list = list(melted_sorted.itertuples(index=False, name=None))[:10]\n",
    "            # Calculate the proportion of confirmed genes in the top k of the ranked pairs\n",
    "                top_10 = [(input, target) for input, target, _ in ranked_list]\n",
    "                true_pairs = set(tuple(x) for x in gt.values)\n",
    "                matches = sum(1 for pair in top_10 if pair in true_pairs)\n",
    "                prop = matches / len(top_10)\n",
    "                total += prop\n",
    "\n",
    "            print(f\"DS{i}\", method, total / 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Pearson ranking tests with proportion 0.1\n",
      "DS1 Clean 0.196\n",
      "DS1 Noisy 0.004\n",
      "DS1 SAUCIE 0.002\n",
      "DS1 scScope 0.0\n",
      "DS1 DeepImpute 0.0\n",
      "DS1 MAGIC 0.0\n",
      "DS2 Clean 0.308\n",
      "DS2 Noisy 0.0\n",
      "DS2 SAUCIE 0.0\n",
      "DS2 scScope 0.0\n",
      "DS2 DeepImpute 0.0\n",
      "DS2 MAGIC 0.0\n",
      "DS3 Clean 0.14799999999999996\n",
      "DS3 Noisy 0.004\n",
      "DS3 SAUCIE 0.0\n",
      "DS3 scScope 0.0\n",
      "DS3 DeepImpute 0.0\n",
      "DS3 MAGIC 0.002\n",
      "Running Pearson ranking tests with proportion 0.2\n",
      "DS1 Clean 0.44000000000000017\n",
      "DS1 Noisy 0.04\n",
      "DS1 SAUCIE 0.004\n",
      "DS1 scScope 0.0\n",
      "DS1 DeepImpute 0.0\n",
      "DS1 MAGIC 0.0\n",
      "DS2 Clean 0.4000000000000001\n",
      "DS2 Noisy 0.002\n",
      "DS2 SAUCIE 0.0\n",
      "DS2 scScope 0.002\n",
      "DS2 DeepImpute 0.0\n",
      "DS2 MAGIC 0.0\n",
      "DS3 Clean 0.3459999999999999\n",
      "DS3 Noisy 0.004\n",
      "DS3 SAUCIE 0.0\n",
      "DS3 scScope 0.0\n",
      "DS3 DeepImpute 0.006000000000000001\n",
      "DS3 MAGIC 0.0\n",
      "Running Pearson ranking tests with proportion 0.3\n",
      "DS1 Clean 0.4660000000000001\n",
      "DS1 Noisy 0.09\n",
      "DS1 SAUCIE 0.006000000000000001\n",
      "DS1 scScope 0.0\n",
      "DS1 DeepImpute 0.0\n",
      "DS1 MAGIC 0.0\n",
      "DS2 Clean 0.41600000000000004\n",
      "DS2 Noisy 0.006000000000000001\n",
      "DS2 SAUCIE 0.0\n",
      "DS2 scScope 0.002\n",
      "DS2 DeepImpute 0.002\n",
      "DS2 MAGIC 0.0\n",
      "DS3 Clean 0.4019999999999999\n",
      "DS3 Noisy 0.006000000000000001\n",
      "DS3 SAUCIE 0.0\n",
      "DS3 scScope 0.0\n",
      "DS3 DeepImpute 0.002\n",
      "DS3 MAGIC 0.0\n"
     ]
    }
   ],
   "source": [
    "run_pearson_ranking(0.1)\n",
    "run_pearson_ranking(0.2)\n",
    "run_pearson_ranking(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero_imputation_venv",
   "language": "python",
   "name": "zero_imputation_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
