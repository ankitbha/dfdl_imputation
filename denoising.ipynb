{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from time import time\n",
    "import concurrent\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from consolidated_runs import run_sergio\n",
    "from SERGIO.SERGIO.sergio import sergio\n",
    "import parallel_utils\n",
    "\n",
    "def new_expected_value_experiment(dataset_id, last_iteration=0, fixed_edge=False, add_edge=True, multiple_edges=False, clean='clean'):\n",
    "    #print(\"Running edge finding experiment\")\n",
    "    gt_file = None\n",
    "    if dataset_id == 1:\n",
    "        gt_file = './SERGIO/data_sets/De-noised_100G_9T_300cPerT_4_DS1/gt_GRN.csv'\n",
    "        target_file = './SERGIO/data_sets/De-noised_100G_9T_300cPerT_4_DS1/Interaction_cID_4.txt'\n",
    "        regs_path = './SERGIO/data_sets/De-noised_100G_9T_300cPerT_4_DS1/Regs_cID_4.txt'\n",
    "        n_genes = 100\n",
    "    elif dataset_id == 2:\n",
    "        gt_file = './SERGIO/data_sets/De-noised_400G_9T_300cPerT_5_DS2/gt_GRN.csv'\n",
    "        target_file = './SERGIO/data_sets/De-noised_400G_9T_300cPerT_5_DS2/Interaction_cID_5.txt'\n",
    "        regs_path = './SERGIO/data_sets/De-noised_400G_9T_300cPerT_5_DS2/Regs_cID_5.txt'\n",
    "        n_genes = 400\n",
    "    elif dataset_id == 3:\n",
    "        gt_file = 'SERGIO/data_sets/De-noised_1200G_9T_300cPerT_6_DS3/gt_GRN.csv'\n",
    "        target_file = './SERGIO/data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Interaction_cID_6.txt'\n",
    "        regs_path = './SERGIO/data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Regs_cID_6.txt'\n",
    "        n_genes = 1200\n",
    "    reg_df = pd.read_csv(regs_path, header=None)\n",
    "    master_regs = [int(m) for m in reg_df[0].values]\n",
    "    true_pearson = pd.DataFrame()\n",
    "    gt = pd.read_csv(gt_file, header=None)\n",
    "    imp_dir = os.path.join(os.getcwd(), 'imputations')\n",
    "    load_dir = os.path.join(imp_dir, f'DS{dataset_id}')\n",
    "    ranks = []\n",
    "    means = []\n",
    "\n",
    "    experiment_dir = os.path.join(os.getcwd(), f'experiments/mean_diffs/DS{dataset_id}')\n",
    "    if not os.path.exists(experiment_dir):\n",
    "        os.makedirs(experiment_dir)\n",
    "    experiment_file = os.path.join(experiment_dir, f\"DS{dataset_id}_mean_diff_experiment.csv\")\n",
    "    if fixed_edge:\n",
    "        experiment_file = os.path.join(experiment_dir, f\"DS{dataset_id}_mean_diff_experiment_fixed_edge.csv\")\n",
    "    if os.path.exists(experiment_file) and last_iteration != 0:\n",
    "        df = pd.read_csv(experiment_file)\n",
    "        ranks = df['Rank'].values.tolist()\n",
    "        means = df['Correlation'].values.tolist()\n",
    "\n",
    "    #run_sergio(target_file, regs_path, dataset_id, file_extension='')\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for iteration in range(last_iteration, 51):      \n",
    "            file_extension = ''                          \n",
    "            file_extension = f\"_iter{iteration}\"\n",
    "            # Get number of genes to choose a target\n",
    "            if iteration != 0:\n",
    "                futures.append(executor.submit(parallel_utils.new_mean_process_iteration, iteration, target_file, regs_path, master_regs, load_dir, add_edge, multiple_edges, imp_dir, dataset_id, file_extension, clean))\n",
    "        \n",
    "        iter = 0\n",
    "        chosen_pairs = []\n",
    "        chosen_pair_filename = os.path.join(experiment_dir, f\"DS{dataset_id}_chosen_pairs.csv\")\n",
    "        expression_data = {}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            expr, clean_data, chosen_targets = future.result()\n",
    "            expression_data['expr'] = expr\n",
    "            expression_data['clean'] = clean_data\n",
    "            # ranks, final_ranks, temp_target, f_ext, ds_iter = future.result()\n",
    "            # #print(chosen_pair, rank)\n",
    "            # experiment_file_ranks = experiment_file.replace('.csv', f'_iter{ds_iter}.csv')\n",
    "            # ranks.to_csv(experiment_file_ranks, index=False)\n",
    "            # #print(final_ranks)\n",
    "            # for tup in final_ranks:\n",
    "            #     chosen_pair = (tup[0], tup[1])\n",
    "            #     add = tup[2]\n",
    "            #     rank = ranks.index.get_loc(chosen_pair[1]) + 1\n",
    "            #     if not add:\n",
    "            #         rank = n_genes - rank\n",
    "            #     # print(chosen_pair[0], chosen_pair[1], rank, add)\n",
    "            #     target_val = tup[4]\n",
    "            #     target_val = ranks.loc[chosen_pair[1]]['mean']\n",
    "            #     overall_mean = np.mean(ranks['mean'])\n",
    "            #     chosen_pairs.append([chosen_pair[0], chosen_pair[1], rank, target_val, overall_mean, add, ds_iter])\n",
    "        #     if ds_iter % 10 != 0:\n",
    "        #         #print(temp_target)\n",
    "        #         os.remove(temp_target)\n",
    "        #         os.remove(os.path.join(experiment_dir, f\"DS{dataset_id}_mean_diff_experiment_iter{ds_iter}.csv\"))\n",
    "        #         os.remove(os.path.join(imp_dir, f'DS{dataset_id}', f\"DS6_clean{f_ext}.npy\"))\n",
    "        #         os.remove(os.path.join(imp_dir, f'DS{dataset_id}', f\"DS6_noisy{f_ext}.npy\"))\n",
    "        #     os.remove(os.path.join(imp_dir, f'DS{dataset_id}', f\"DS6_expr{f_ext}.npy\"))\n",
    "        #     os.remove(os.path.join(imp_dir, f'DS{dataset_id}', f\"DS6_clean_counts{f_ext}.npy\"))\n",
    "        #     iter += 1\n",
    "        # chosen_pair_df = pd.DataFrame(chosen_pairs)\n",
    "        # chosen_pair_df.to_csv(chosen_pair_filename, index=False)\n",
    "        n_bins, n_sc = 9, 300\n",
    "        if dataset_id == 1:\n",
    "            n_genes = 100\n",
    "        if dataset_id == 2:\n",
    "            n_genes = 400\n",
    "        sim = sergio(\n",
    "            number_genes=n_genes, \n",
    "            number_bins = n_bins, \n",
    "            number_sc = n_sc,\n",
    "            # In paper\n",
    "            noise_params = 1,\n",
    "            # In paper\n",
    "            decays=0.8, \n",
    "            sampling_state=15, \n",
    "            noise_type='dpd')\n",
    "        \n",
    "        outliers = [sim.outlier_effect(expression_data['expr'], outlier_prob = 0.01, mean = 5, scale = 1) for i in range(1)]\n",
    "        expression_data['outliers'] = outliers\n",
    "        return expression_data, chosen_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:From /Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "100%|██████████| 1/1 [00:49<00:00, 49.20s/it]\n"
     ]
    }
   ],
   "source": [
    "expr_data, chosen_targets = new_expected_value_experiment(dataset_id=1, last_iteration=50, fixed_edge=False, add_edge=True, multiple_edges=False, clean='noisy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = expr_data['outliers']\n",
    "\n",
    "reshaped = [np.concatenate(ds, axis = 1) for ds in datasets]\n",
    "stacked_data = np.stack(reshaped, axis=2)\n",
    "consensus_data = np.median(stacked_data, axis=2)\n",
    "\n",
    "np.corrcoef(consensus_data.flatten(), expr_data['clean'].flatten())[0, 1]\n",
    "# median_expressions = [np.median(np.abs(norm), axis=2) for norm in datasets]\n",
    "\n",
    "# stacked_data = np.stack(median_expressions)\n",
    "\n",
    "# consensus_data = np.median(stacked_data, axis=0)\n",
    "\n",
    "# variability_scores = np.std(consensus_data, axis=0)\n",
    "\n",
    "# gene_ranks = np.argsort(-variability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGenes_ = 100\n",
    "nBins_ = 9\n",
    "nSC_ = 300\n",
    "from scipy.stats import norm\n",
    "\n",
    "def outlier_effect(scData, outlier_prob, mean, scale):\n",
    "    out_indicator = np.random.binomial(n = 1, p = outlier_prob, size = nGenes_)\n",
    "    outlierGenesIndx = np.where(out_indicator == 1)[0]\n",
    "    numOutliers = len(outlierGenesIndx)\n",
    "\n",
    "    #### generate outlier factors ####\n",
    "    outFactors = np.random.lognormal(mean = mean, sigma = scale, size = numOutliers)\n",
    "    ##################################\n",
    "\n",
    "    scData = np.concatenate(scData, axis = 1)\n",
    "    for i, gIndx in enumerate(outlierGenesIndx):\n",
    "        scData[gIndx,:] = scData[gIndx,:] * outFactors[i]\n",
    "\n",
    "    return np.split(scData, nBins_, axis = 1)\n",
    "\n",
    "def lib_size_effect(self, scData, mean, scale):\n",
    "        \"\"\"\n",
    "        This functions adjusts the mRNA levels in each cell seperately to mimic\n",
    "        the library size effect. To adjust mRNA levels, cell-specific factors are sampled\n",
    "        from a log-normal distribution with given mean and scale.\n",
    "\n",
    "        scData: the simulated data representing mRNA levels (concentrations);\n",
    "        np.array (#bins * #genes * #cells)\n",
    "\n",
    "        mean: mean for log-normal distribution\n",
    "\n",
    "        var: var for log-normal distribution\n",
    "\n",
    "        returns libFactors ( np.array(nBin, nCell) )\n",
    "        returns modified single cell data ( np.array(nBin, nGene, nCell) )\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO make sure that having bins does not intefere with this implementation\n",
    "        ret_data = []\n",
    "\n",
    "        libFactors = np.random.lognormal(mean = mean, sigma = scale, size = (nBins_, nSC_))\n",
    "        for binExprMatrix, binFactors in zip(scData, libFactors):\n",
    "            normalizFactors = np.sum(binExprMatrix, axis = 0 )\n",
    "            binFactors = np.true_divide(binFactors, normalizFactors)\n",
    "            binFactors = binFactors.reshape(1, nSC_)\n",
    "            binFactors = np.repeat(binFactors, nGenes_, axis = 0)\n",
    "\n",
    "            ret_data.append(np.multiply(binExprMatrix, binFactors))\n",
    "\n",
    "\n",
    "        return libFactors, np.array(ret_data)\n",
    "\n",
    "def outlier_denoise(dataset_list):\n",
    "    reshaped = [np.concatenate(ds, axis = 1) for ds in dataset_list]\n",
    "    stacked_data = np.stack(reshaped, axis=2)\n",
    "    consensus_data = np.median(stacked_data, axis=2)\n",
    "    return consensus_data\n",
    "\n",
    "def count_normalization(data):\n",
    "    normalized_data = []\n",
    "    for expression_matrix in data:\n",
    "        totals = np.sum(expression_matrix, axis=0)\n",
    "        scale_factor = np.median(totals)\n",
    "        normed = expression_matrix / totals * scale_factor\n",
    "        normalized_data.append(normed)\n",
    "    return np.array(normalized_data)\n",
    "\n",
    "def fit_gaussian(row):\n",
    "    mu, sigma = norm.fit(row)\n",
    "    return mu, sigma\n",
    "\n",
    "def dropout_denoise(data, id):\n",
    "    ids = range(len(data))\n",
    "    random_other_id = random.choice([i for i in ids if i != id])\n",
    "    stacked_data = np.stack(data)\n",
    "    output_array = stacked_data[id]\n",
    "\n",
    "    excluded_data = np.delete(stacked_data, id, axis=0)\n",
    "\n",
    "    # concatenated = np.concatenate(stacked_data, axis=0)\n",
    "    # concatenated = concatenated.reshape(stacked_data.shape[2], -1)\n",
    "    # concatenated = pd.DataFrame(concatenated)\n",
    "    # gaussian_params = concatenated.apply(fit_gaussian, axis=1)\n",
    "    # print(gaussian_params)\n",
    "    concatenated = np.concatenate(excluded_data, axis=1)\n",
    "    concatenated = np.concatenate(concatenated, axis=1)\n",
    "    concatenated = pd.DataFrame(concatenated)\n",
    "    print(concatenated.shape)\n",
    "    gaussian_params = concatenated.apply(fit_gaussian, axis=1)\n",
    "    #print(gaussian_params)\n",
    "    for i in range(stacked_data.shape[1]):\n",
    "        for j in range(stacked_data.shape[2]):\n",
    "            for k in range(stacked_data.shape[3]):\n",
    "                # print(stacked_data[:, i, j, k])\n",
    "                if output_array[i, j, k] == 0:\n",
    "                    value = stacked_data[random_other_id, i, j, k]\n",
    "                    if value == 0:\n",
    "                        gaussian_params_row = gaussian_params.iloc[j + random_other_id * stacked_data.shape[2]]\n",
    "                        mu, sigma = gaussian_params_row\n",
    "                        gaussian_sample = norm.rvs(loc=mu, scale=sigma, size=1)\n",
    "                        output_array[i, j, k] = gaussian_sample\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m binaries \u001b[38;5;241m=\u001b[39m [sim\u001b[38;5;241m.\u001b[39mdropout_indicator(lib_data[\u001b[38;5;241m1\u001b[39m], shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, percentile \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m45\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m lib_data \u001b[38;5;129;01min\u001b[39;00m library_data]\n\u001b[1;32m     27\u001b[0m final_expression \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mmultiply(lib_data[\u001b[38;5;241m1\u001b[39m], binary) \u001b[38;5;28;01mfor\u001b[39;00m lib_data, binary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(library_data, binaries)]\n\u001b[0;32m---> 28\u001b[0m dropout_denoised \u001b[38;5;241m=\u001b[39m [dropout_denoise(final_expression, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(final_expression))]\n\u001b[1;32m     29\u001b[0m library_denoised \u001b[38;5;241m=\u001b[39m [count_normalization(dropout_denoised) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(library_data))]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outliers)):\n",
      "Cell \u001b[0;32mIn[162], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m binaries \u001b[38;5;241m=\u001b[39m [sim\u001b[38;5;241m.\u001b[39mdropout_indicator(lib_data[\u001b[38;5;241m1\u001b[39m], shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, percentile \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m45\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m lib_data \u001b[38;5;129;01min\u001b[39;00m library_data]\n\u001b[1;32m     27\u001b[0m final_expression \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mmultiply(lib_data[\u001b[38;5;241m1\u001b[39m], binary) \u001b[38;5;28;01mfor\u001b[39;00m lib_data, binary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(library_data, binaries)]\n\u001b[0;32m---> 28\u001b[0m dropout_denoised \u001b[38;5;241m=\u001b[39m [\u001b[43mdropout_denoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_expression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(final_expression))]\n\u001b[1;32m     29\u001b[0m library_denoised \u001b[38;5;241m=\u001b[39m [count_normalization(dropout_denoised) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(library_data))]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outliers)):\n",
      "Cell \u001b[0;32mIn[161], line 97\u001b[0m, in \u001b[0;36mdropout_denoise\u001b[0;34m(data, id)\u001b[0m\n\u001b[1;32m     95\u001b[0m value \u001b[38;5;241m=\u001b[39m stacked_data[random_other_id, i, j, k]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     gaussian_params_row \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_other_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstacked_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     98\u001b[0m     mu, sigma \u001b[38;5;241m=\u001b[39m gaussian_params_row\n\u001b[1;32m     99\u001b[0m     gaussian_sample \u001b[38;5;241m=\u001b[39m norm\u001b[38;5;241m.\u001b[39mrvs(loc\u001b[38;5;241m=\u001b[39mmu, scale\u001b[38;5;241m=\u001b[39msigma, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1656\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.9/site-packages/pandas/core/indexing.py:1589\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1587\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "from SERGIO.SERGIO.sergio import sergio\n",
    "\n",
    "n_bins, n_sc = 9, 300\n",
    "dataset_id = 1\n",
    "if dataset_id == 1:\n",
    "    n_genes = 100\n",
    "if dataset_id == 2:\n",
    "    n_genes = 400\n",
    "sim = sergio(\n",
    "    number_genes=n_genes, \n",
    "    number_bins = n_bins, \n",
    "    number_sc = n_sc,\n",
    "    # In paper\n",
    "    noise_params = 1,\n",
    "    # In paper\n",
    "    decays=0.8, \n",
    "    sampling_state=15, \n",
    "    noise_type='dpd')\n",
    "\n",
    "expr_raw = expr_data['expr']\n",
    "\n",
    "corr_coeffs = []\n",
    "for i in range(100):\n",
    "    outliers = [sim.outlier_effect(expr_raw, outlier_prob = 0.01, mean = 5, scale = 1) for i in range(10)]\n",
    "    library_data = [sim.lib_size_effect(expr_outlier, mean = 0, scale = 0.1) for expr_outlier in outliers]\n",
    "    binaries = [sim.dropout_indicator(lib_data[1], shape = 8, percentile = 45) for lib_data in library_data]\n",
    "    final_expression = [np.multiply(lib_data[1], binary) for lib_data, binary in zip(library_data, binaries)]\n",
    "    dropout_denoised = [dropout_denoise(final_expression, i) for i in range(len(final_expression))]\n",
    "    library_denoised = [count_normalization(dropout_denoised) for i in range(len(library_data))]\n",
    "    for i in range(len(outliers)):\n",
    "        outlier = outliers[i]\n",
    "        library = library_data[i][1]\n",
    "        outlier = np.array(outlier)\n",
    "        print(\"Raw expression to outlier noise correlation\")\n",
    "        print(np.corrcoef(expr_raw.flatten(), outlier.flatten())[0, 1])\n",
    "        print(\"Raw expression to library noise correlation\")\n",
    "        print(np.corrcoef(expr_raw.flatten(), library.flatten())[0, 1])\n",
    "        print(\"Raw expression to dropout correlation\")\n",
    "        print(np.corrcoef(expr_raw.flatten(), final_expression[i].flatten())[0, 1])\n",
    "        print(\"======= Data reconstruction =======\")\n",
    "        print(\"Dropout noise to library noise correlation\")\n",
    "        print(np.corrcoef(final_expression[i].flatten(), library.flatten())[0, 1])\n",
    "        print(\"De-noised dropout to library noise correlation\")\n",
    "        print(np.corrcoef(dropout_denoised[i].flatten(), library.flatten())[0, 1])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    break\n",
    "    normed = count_normalization(lib_data)\n",
    "    denoised = outlier_denoise([normed])\n",
    "\n",
    "    print(\"Outlier noise to raw expression data correlation\")\n",
    "    print(np.corrcoef(np.array(expr_outlier).flatten(), np.array(expr_raw).flatten())[0, 1])\n",
    "    out_denoise = outlier_denoise([expr_outlier])\n",
    "    print(\"Denoised to outlier noise correlation\")\n",
    "    print(np.corrcoef(out_denoise.flatten(), np.array(expr_outlier).flatten())[0, 1])\n",
    "    print(\"Denoised to raw expression data correlation\")\n",
    "    print(np.corrcoef(out_denoise.flatten(), np.array(expr_raw).flatten())[0, 1])\n",
    "\n",
    "    print(\"library noise to outlier noise correlation\")\n",
    "    print(np.corrcoef(np.array(lib_data).flatten(), np.array(expr_outlier).flatten())[0, 1])\n",
    "    print(\"library noise to raw expression data correlation\")\n",
    "    print(np.corrcoef(np.array(lib_data).flatten(), np.array(expr_raw).flatten())[0, 1])\n",
    "\n",
    "    print(\"Normalized to library noise correlation\")\n",
    "    print(np.corrcoef(normed.flatten(), np.array(lib_data).flatten())[0, 1])\n",
    "    print(\"Normalized to outlier noise (pre library) correlation\")\n",
    "    print(np.corrcoef(normed.flatten(), np.array(expr_outlier).flatten())[0, 1])\n",
    "    print(\"Normalized to raw expression data correlation\")\n",
    "    print(np.corrcoef(normed.flatten(), np.array(expr_raw).flatten())[0, 1])\n",
    "\n",
    "    corr_coeffs.append(np.corrcoef(denoised.flatten(), np.array(expr_raw).flatten())[0, 1])\n",
    "    \n",
    "np.mean(corr_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero_imputation_venv",
   "language": "python",
   "name": "zero_imputation_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
