{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_experiments import run_simulation\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "sys.path.append(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    outputs = []\n",
    "    ret_df = None\n",
    "    for dataset in [3]:\n",
    "        if not os.path.exists(f\"./zero_imputation_experiments/DS{dataset}/\"):\n",
    "            os.makedirs(f\"./zero_imputation_experiments/DS{dataset}/\")\n",
    "        # Run for first iteration to prevent race condition\n",
    "        res = run_simulation(\n",
    "            dataset=dataset,\n",
    "            sergio=True,\n",
    "            saucie=True, \n",
    "            scScope=True, \n",
    "            deepImpute=True, \n",
    "            magic=True, \n",
    "            genie=True,\n",
    "            arboreto=False,\n",
    "            pearson=False,\n",
    "            roc=True,\n",
    "            precision_recall_k=False,\n",
    "            run_with_regs=False,\n",
    "            iteration=0\n",
    "        )\n",
    "        clear_output()\n",
    "        if ret_df is None:\n",
    "            ret_df = pd.DataFrame(columns=res.keys())\n",
    "        new_df = pd.DataFrame([res], columns=res.keys())\n",
    "        ret_df = pd.concat([ret_df, new_df], ignore_index=True)\n",
    "        #write to temp file\n",
    "        ret_df.to_csv(\"zero_imputation_experiments/imputation_results.csv\", index=False)\n",
    "        with ProcessPoolExecutor(max_workers=3) as executor:\n",
    "            futures = []\n",
    "            for i in range(1, 30):\n",
    "                futures.append(executor.submit(run_simulation, \n",
    "                        dataset=dataset,\n",
    "                        sergio=(i == 0),\n",
    "                        saucie=True, \n",
    "                        scScope=True, \n",
    "                        deepImpute=True, \n",
    "                        magic=True, \n",
    "                        genie=True,\n",
    "                        arboreto=False,\n",
    "                        pearson=False,\n",
    "                        roc=True,\n",
    "                        precision_recall_k=False,\n",
    "                        run_with_regs=False,\n",
    "                        iteration=i\n",
    "                    ))\n",
    "                clear_output()\n",
    "            for future in tqdm(futures):\n",
    "                res = future.result()\n",
    "                clear_output(wait=True)\n",
    "                if ret_df is None:\n",
    "                    ret_df = pd.DataFrame(columns=res.keys())\n",
    "                new_df = pd.DataFrame([res], columns=res.keys())\n",
    "                ret_df = pd.concat([ret_df, new_df], ignore_index=True)\n",
    "                #write to temp file\n",
    "                ret_df.to_csv(\"zero_imputation_experiments/imputation_results.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Imputation Methods Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dataset in [1,2,3]:\n",
    "    if not os.path.exists(f\"./zero_imputation_experiments/DS{dataset}/\"):\n",
    "        os.makedirs(f\"./zero_imputation_experiments/DS{dataset}/\")\n",
    "    if not os.path.exists(f\"./zero_imputation_experiments/DS{dataset}/DS6_noisy.npy\"):\n",
    "        res = run_simulation(\n",
    "            dataset=dataset,\n",
    "            sergio=True,\n",
    "            saucie=False, \n",
    "            scScope=False, \n",
    "            deepImpute=False, \n",
    "            magic=False, \n",
    "            genie=False,\n",
    "            arboreto=False,\n",
    "            pearson=False,\n",
    "            roc=False,\n",
    "            precision_recall_k=False,\n",
    "            run_with_regs=False,\n",
    "            iteration=0\n",
    "        )\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_utils import run_scvi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_target_regs(dataset):\n",
    "    if dataset == 1:   \n",
    "        target_file = './SERGIO/data_sets/De-noised_100G_9T_300cPerT_4_DS1/Interaction_cID_4.txt'\n",
    "        regs_path = './SERGIO/data_sets/De-noised_100G_9T_300cPerT_4_DS1/Regs_cID_4.txt'\n",
    "    elif dataset == 2:\n",
    "        target_file = './SERGIO/data_sets/De-noised_400G_9T_300cPerT_5_DS2/Interaction_cID_5.txt'\n",
    "        regs_path = './SERGIO/data_sets/De-noised_400G_9T_300cPerT_5_DS2/Regs_cID_5.txt'\n",
    "    else:\n",
    "        target_file = './SERGIO/data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Interaction_cID_6.txt'\n",
    "        regs_path = './SERGIO/data_sets/De-noised_1200G_9T_300cPerT_6_DS3/Regs_cID_6.txt'\n",
    "    return target_file, regs_path    \n",
    "\n",
    "def scvi_impute():\n",
    "    ret_df = None\n",
    "    for dataset in [1, 2, 3]:\n",
    "        save_path = f\"./zero_imputation_experiments/DS{dataset}/\"\n",
    "        y = np.load(save_path + \"/DS6_noisy.npy\")\n",
    "        target_file, regs_path = fetch_target_regs(dataset)\n",
    "        with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "            futures = []\n",
    "            for i in range(8):\n",
    "                futures.append(executor.submit(run_scvi, \n",
    "                    data=y, \n",
    "                    save_path=save_path, \n",
    "                    it=i, \n",
    "                    file_extension=f\"_iter{i}\",\n",
    "                    target_file=target_file\n",
    "                ))\n",
    "                clear_output()\n",
    "            for future in tqdm(futures):\n",
    "                vim, it = future.result()\n",
    "                res = {\n",
    "                    \"dataset\": dataset,\n",
    "                    \"method\": \"scvi\",\n",
    "                    \"roc\": vim,\n",
    "                    \"iteration\": it }\n",
    "                if ret_df is None:\n",
    "                    ret_df = pd.DataFrame(columns=res.keys())\n",
    "                new_df = pd.DataFrame([res], columns=res.keys())\n",
    "                ret_df = pd.concat([ret_df, new_df], ignore_index=True)\n",
    "                ret_df.to_csv(\"zero_imputation_experiments/scvi_imputation_results.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]GPU available: True (mps), used: False\n",
      "GPU available: True (mps), used: False\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [03:37<00:00,  1.84it/s, v_num=1, train_loss_step=275, train_loss_epoch=223]\n",
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [03:39<00:00,  1.83it/s, v_num=1, train_loss_step=356, train_loss_epoch=234]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n",
      "Epoch 400/400: 100%|██████████| 400/400 [03:51<00:00,  1.73it/s, v_num=1, train_loss_step=146, train_loss_epoch=240]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n",
      "Epoch 400/400: 100%|██████████| 400/400 [03:54<00:00,  1.70it/s, v_num=1, train_loss_step=189, train_loss_epoch=225]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n",
      "Elapsed time: 6879.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 224486.11it/s]\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/400:   4%|▎         | 14/400 [00:20<08:33,  1.33s/it, v_num=1, train_loss_step=369, train_loss_epoch=262]Elapsed time: 6947.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 297865.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/400:  10%|▉         | 39/400 [00:51<06:26,  1.07s/it, v_num=1, train_loss_step=373, train_loss_epoch=256]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [2:00:02<14:00:16, 7202.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/400:  10%|█         | 40/400 [00:52<06:34,  1.10s/it, v_num=1, train_loss_step=182, train_loss_epoch=255]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/400:   1%|▏         | 5/400 [00:05<07:19,  1.11s/it, v_num=1, train_loss_step=448, train_loss_epoch=273]5]Elapsed time: 6971.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 320398.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/400:   6%|▌         | 23/400 [00:25<06:11,  1.01it/s, v_num=1, train_loss_step=196, train_loss_epoch=255]Elapsed time: 6987.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 258437.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400:   0%|          | 0/400 [00:00<?, ?it/s] 1.04s/it, v_num=1, train_loss_step=276, train_loss_epoch=255]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/400:   4%|▎         | 14/400 [00:14<06:20,  1.01it/s, v_num=1, train_loss_step=177, train_loss_epoch=271]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [2:00:44<2:36:54, 1883.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400:   0%|          | 0/400 [00:00<?, ?it/s] 1.07s/it, v_num=1, train_loss_step=318, train_loss_epoch=269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/joshuaweiner/Desktop/Folders/Projects/zero_imputation/venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/400: 100%|██████████| 400/400 [06:55<00:00,  1.04s/it, v_num=1, train_loss_step=169, train_loss_epoch=239]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n",
      "Epoch 400/400: 100%|██████████| 400/400 [06:52<00:00,  1.03s/it, v_num=1, train_loss_step=177, train_loss_epoch=235]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n",
      "Epoch 400/400: 100%|██████████| 400/400 [07:16<00:00,  1.09s/it, v_num=1, train_loss_step=289, train_loss_epoch=247]\n",
      "Epoch 384/400:  96%|█████████▌| 383/400 [07:01<00:29,  1.75s/it, v_num=1, train_loss_step=479, train_loss_epoch=232]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n",
      "Epoch 400/400: 100%|██████████| 400/400 [07:38<00:00,  1.15s/it, v_num=1, train_loss_step=297, train_loss_epoch=232]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree method: RF\n",
      "K: sqrt\n",
      "Number of trees: 100\n",
      "\n",
      "\n",
      "running jobs on 12 threads\n",
      "Elapsed time: 9251.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 373655.17it/s]\n",
      " 62%|██████▎   | 5/8 [4:40:52<2:49:14, 3384.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 9261.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 405334.91it/s]\n",
      " 75%|███████▌  | 6/8 [4:41:42<1:22:58, 2489.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 9235.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 340241.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 9281.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [00:00<00:00, 373802.13it/s]\n",
      "100%|██████████| 8/8 [4:42:34<00:00, 2119.26s/it]  \n"
     ]
    }
   ],
   "source": [
    "scvi_impute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero_imputation_venv",
   "language": "python",
   "name": "zero_imputation_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
